{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/benjaminbrown038/Machine-Learning/blob/main/PyTorch-Models-Data-Handling/FastAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMrexOB8OEGI"
   },
   "source": [
    "# Computer Vision and Data Handling in PyTorch\n",
    " 1. [Imports](#imports)\n",
    " 2. [Data](#data)\n",
    " 3. [Architectures](#architectures)\n",
    " 4. [Training Loop](#training)\n",
    " 5. [Testing Loop](#testing)\n",
    " 6. [Functions](#functions)\n",
    " 7. [Classes](#classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py5Kc2oR_pJ2"
   },
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/trey/Library/Python/3.8/lib/python/site-packages (1.12.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/trey/Library/Python/3.8/lib/python/site-packages (from torch) (4.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastai\n",
      "  Downloading fastai-2.7.11-py3-none-any.whl (232 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from fastai) (0.24.1)\n",
      "Collecting fastprogress>=0.2.4\n",
      "  Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from fastai) (1.6.2)\n",
      "Requirement already satisfied: requests in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from fastai) (2.25.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from fastai) (5.4.1)\n",
      "Collecting fastdownload<2,>=0.0.5\n",
      "  Downloading fastdownload-0.0.7-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pillow>6.0.0 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from fastai) (8.2.0)\n",
      "Collecting spacy<4\n",
      "  Downloading spacy-3.5.0-cp38-cp38-win_amd64.whl (12.6 MB)\n",
      "Requirement already satisfied: pandas in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from fastai) (1.2.4)\n",
      "Requirement already satisfied: torch<1.14,>=1.7 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from fastai) (1.13.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from fastai) (3.3.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from fastai) (20.9)\n",
      "Collecting torchvision>=0.8.2\n",
      "  Downloading torchvision-0.14.1-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Requirement already satisfied: pip in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from fastai) (21.3)\n",
      "Collecting fastcore<1.6,>=1.4.5\n",
      "  Downloading fastcore-1.5.28-py3-none-any.whl (67 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp38-cp38-win_amd64.whl (96 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp38-cp38-win_amd64.whl (18 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (1.20.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (52.0.0.post20210125)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp38-cp38-win_amd64.whl (30 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (2.11.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.5-cp38-cp38-win_amd64.whl (2.2 MB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.7-cp38-cp38-win_amd64.whl (1.3 MB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp38-cp38-win_amd64.whl (481 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from spacy<4->fastai) (4.59.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from packaging->fastai) (2.4.7)\n",
      "Collecting typing-extensions>=4.2.0\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from requests->fastai) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from requests->fastai) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from requests->fastai) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from requests->fastai) (4.0.0)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp38-cp38-win_amd64.whl (7.0 MB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<4->fastai) (7.1.2)\n",
      "Collecting colorama>=0.4.6\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from jinja2->spacy<4->fastai) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from matplotlib->fastai) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->fastai) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from pandas->fastai) (2021.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from scikit-learn->fastai) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from scikit-learn->fastai) (2.1.0)\n",
      "Installing collected packages: typing-extensions, catalogue, srsly, pydantic, murmurhash, cymem, colorama, wasabi, typer, smart-open, preshed, confection, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, fastprogress, fastcore, torchvision, spacy, fastdownload, fastai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 colorama-0.4.6 confection-0.0.4 cymem-2.0.7 fastai-2.7.11 fastcore-1.5.28 fastdownload-0.0.7 fastprogress-1.0.3 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.5 smart-open-6.3.0 spacy-3.5.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.7 torchvision-0.14.1 typer-0.7.0 typing-extensions-4.5.0 wasabi-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylint 2.7.4 requires astroid<2.7,>=2.5.2, but you have astroid 2.5 which is incompatible.\n",
      "WARNING: You are using pip version 21.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\BenBrown\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTIR5FGBOEGK",
    "outputId": "eba0c43a-ab85-4f0f-9391-b754ee378c19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\BenBrown\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ray\n",
      "  Downloading ray-2.2.0-cp38-cp38-win_amd64.whl (20.8 MB)\n",
      "Requirement already satisfied: grpcio>=1.32.0 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from ray) (1.36.1)\n",
      "Requirement already satisfied: attrs in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from ray) (20.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from ray) (3.0.12)\n",
      "Collecting frozenlist\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-win_amd64.whl (34 kB)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from ray) (1.20.1)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from ray) (3.17.2)\n",
      "Collecting virtualenv>=20.0.24\n",
      "  Downloading virtualenv-20.19.0-py3-none-any.whl (8.7 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from ray) (2.25.1)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from ray) (3.2.0)\n",
      "Collecting aiosignal\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from ray) (1.0.2)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from ray) (7.1.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from ray) (5.4.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from grpcio>=1.32.0->ray) (1.15.0)\n",
      "Collecting distlib<1,>=0.3.6\n",
      "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
      "Collecting platformdirs<4,>=2.4\n",
      "  Downloading platformdirs-3.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from jsonschema->ray) (0.17.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from jsonschema->ray) (52.0.0.post20210125)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from requests->ray) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from requests->ray) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from requests->ray) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\benbrown\\anaconda3\\lib\\site-packages (from requests->ray) (2.10)\n",
      "Installing collected packages: platformdirs, frozenlist, filelock, distlib, virtualenv, aiosignal, ray\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.0.12\n",
      "    Uninstalling filelock-3.0.12:\n",
      "      Successfully uninstalled filelock-3.0.12\n",
      "Successfully installed aiosignal-1.3.1 distlib-0.3.6 filelock-3.9.0 frozenlist-1.3.3 platformdirs-3.0.0 ray-2.2.0 virtualenv-20.19.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'message_types_by_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e1208d3a336d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' pip3 install ray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malexnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minception_v3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ray\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"2.2.0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raylet\u001b[0m  \u001b[1;31m# noqa: E402\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m from ray._raylet import (  # noqa: E402,F401\n",
      "\u001b[1;32mpython\\ray\\_raylet.pyx\u001b[0m in \u001b[0;36minit ray._raylet\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ray\\exceptions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcloudpickle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raylet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mActorID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTaskID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWorkerID\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m from ray.core.generated.common_pb2 import (\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mPYTHON\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mActorDiedErrorContext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ray\\core\\generated\\common_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mruntime_env_common_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msrc_dot_ray_dot_protobuf_dot_runtime__env__common__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ray\\core\\generated\\runtime_env_common_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0m_RUNTIMEENVURIS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDESCRIPTOR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage_types_by_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'RuntimeEnvUris'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0m_RUNTIMEENVCONFIG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDESCRIPTOR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage_types_by_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'RuntimeEnvConfig'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0m_RUNTIMEENVINFO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDESCRIPTOR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage_types_by_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'RuntimeEnvInfo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'message_types_by_name'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.vision.all import *\n",
    "from torch.nn import functional\n",
    "import numpy\n",
    "import torchvision\n",
    "! pip3 install ray\n",
    "import ray\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, alexnet, inception_v3\n",
    "from torch.nn import Sequential\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alst8TCbOEGL"
   },
   "source": [
    "##### Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "DQ1nujBQOEGL",
    "outputId": "79d24e6f-23bf-4cf8-c068-bd41daf9cb91"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
=======
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminbrown038/Machine-Learning/blob/main/PyTorch-Models-Data-Handling/FastAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMrexOB8OEGI"
      },
      "source": [
        "# Computer Vision and Data Handling in PyTorch\n",
        " 1. [Imports](#imports)\n",
        " 2. [Data](#data)\n",
        "    \n",
        "    A. [Augmentation](#aug) \n",
        " 3. [Architectures](#architectures)\n",
        " 4. [Training Loop](#training)\n",
        " 5. [Testing Loop](#testing)\n",
        " 6. [Functions](#functions)\n",
        " 7. [Classes](#classes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py5Kc2oR_pJ2"
      },
      "source": [
        "##### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8FYHftRa7Ld",
        "outputId": "d6bc60ee-37c7-42c5-8958-737de6b70715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "! pip3 install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9mW6HKEa7Le",
        "outputId": "f0d31515-8f51-44c7-c496-ad70cf0b08de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastai in /usr/local/lib/python3.8/dist-packages (2.7.11)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.8/dist-packages (from fastai) (0.0.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from fastai) (6.0)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.8/dist-packages (from fastai) (1.0.3)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.8/dist-packages (from fastai) (3.4.4)\n",
            "Requirement already satisfied: fastcore<1.6,>=1.4.5 in /usr/local/lib/python3.8/dist-packages (from fastai) (1.5.28)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from fastai) (0.14.1+cu116)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fastai) (1.3.5)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.8/dist-packages (from fastai) (7.1.2)\n",
            "Requirement already satisfied: torch<1.14,>=1.7 in /usr/local/lib/python3.8/dist-packages (from fastai) (1.13.1+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fastai) (2.25.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from fastai) (1.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from fastai) (23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from fastai) (1.7.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from fastai) (3.5.3)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (from fastai) (22.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (3.0.12)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (6.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (1.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (2.0.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (1.10.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (1.22.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (2.4.5)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (8.1.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (1.0.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4->fastai) (0.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fastai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fastai) (4.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<1.14,>=1.7->fastai) (4.5.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai) (4.38.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fastai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fastai) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fastai) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fastai) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.15.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4->fastai) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<4->fastai) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTIR5FGBOEGK",
        "outputId": "d6c67d85-4ad5-47d8-a794-e871e215908d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ray\n",
            "  Downloading ray-2.3.0-cp38-cp38-manylinux2014_x86_64.whl (58.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting virtualenv>=20.0.24\n",
            "  Downloading virtualenv-20.19.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from ray) (2.25.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.8/dist-packages (from ray) (3.19.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from ray) (3.9.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from ray) (22.2.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from ray) (1.22.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ray) (1.0.4)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.8/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.8/dist-packages (from ray) (1.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from ray) (6.0)\n",
            "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from ray) (1.51.1)\n",
            "Collecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.8/dist-packages (from virtualenv>=20.0.24->ray) (3.0.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->ray) (5.12.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->ray) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->ray) (4.0.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.14.0)\n",
            "Installing collected packages: distlib, virtualenv, ray\n",
            "Successfully installed distlib-0.3.6 ray-2.3.0 virtualenv-20.19.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "from fastai.vision.all import *\n",
        "from torch.nn import functional\n",
        "import numpy\n",
        "import torchvision\n",
        "! pip3 install ray\n",
        "import ray\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50, alexnet, inception_v3\n",
        "from torch.nn import Sequential\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alst8TCbOEGL"
      },
      "source": [
        "##### Data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "DQ1nujBQOEGL",
        "outputId": "b8614f4a-5bcc-4818-872a-42710bf3acd1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='3219456' class='' max='3214948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.14% [3219456/3214948 00:00&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "time = torch.arange(0,20)\n",
        "params = torch.randn(3).requires_grad_()\n",
        "path = untar_data(URLs.MNIST_SAMPLE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfqR_j0Zke1M"
      },
      "source": [
        "##### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmW76pdEZtTz"
      },
      "outputs": [],
      "source": [
        "# image training data sorted in file path\n",
        "threes = (path/'train/3').ls().sorted()\n",
        "sevens = (path/'train/7').ls().sorted()\n",
        "\n",
        "# image training tensors stacked into one tensor \n",
        "stacked_3s = [tensor(Image.open(o)) for o in threes]\n",
        "stacked_7s = [tensor(Image.open(o)) for o in sevens]\n",
        "\n",
        "# image validation data sorted in file path\n",
        "valid_3s = (path/'valid/3').ls().sorted()\n",
        "valid_7s = (path/'valid/7').ls().sorted()\n",
        "\n",
        "# image validation tensors stacked into one tensor \n",
        "stacked_valid3s = [tensor(Image.open(o)) for o in valid_3s]\n",
        "stacked_valid7s = [tensor(Image.open(o)) for o in valid_7s]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5IhJ_hRq9fl"
      },
      "source": [
        "##### Data Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ku9Q614nfAY",
        "outputId": "489dd68a-9433-4e7d-943a-a10aabeca67c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'fastcore.foundation.L'>\n",
            "<class 'fastcore.foundation.L'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'fastcore.foundation.L'>\n",
            "<class 'fastcore.foundation.L'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(type(threes))\n",
        "print(type(sevens))\n",
        "print(type(stacked_3s))\n",
        "print(type(stacked_7s))\n",
        "print(type(valid_3s))\n",
        "print(type(valid_7s))\n",
        "print(type(stacked_valid3s))\n",
        "print(type(stacked_valid7s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iZJokyccWBh"
      },
      "source": [
        "##### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbTrcqdGcWHM"
      },
      "outputs": [],
      "source": [
        "trafos = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Grayscale(num_output_channels=3),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "mnist_train = torchvision.datasets.MNIST(root='\\data',download=True,train=True,transform = trafos)\n",
        "mnist_test = torchvision.datasets.MNIST(root='\\data',download=True,train = False,transform = trafos)\n",
        "mnist_train_dataloader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
        "mnist_test_dataloader = DataLoader(mnist_test, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "cifar_train = torchvision.datasets.MNIST(root='\\data',download=True,train=True,transform = trafos)\n",
        "cifar_test = torchvision.datasets.MNIST(root='\\data',download=True,train = False,transform = trafos)\n",
        "cifar_train_dataloader = DataLoader(cifar_train, batch_size=64, shuffle=True)\n",
        "cifar_test_dataloader = DataLoader(cifar_test, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWv5fAYjceM9"
      },
      "source": [
        "##### Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOvsH_HoceUO",
        "outputId": "82b6e209-efdc-4204-9881-95931297ea90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set of MNIST:  \n",
            "\n",
            "Type of object that holds training data:\n",
            "<class 'torchvision.datasets.mnist.MNIST'> \n",
            "\n",
            "<class 'tuple'>\n",
            "Length of Tuple: in Torchvision Object:  \n",
            " 2 \n",
            "\n",
            "Image: \n",
            "<class 'torch.Tensor'> \n",
            "\n",
            "Shape of Image: \n",
            " torch.Size([1, 28, 28]) \n",
            "\n",
            "Class: \n",
            "<class 'int'>\n",
            "Length of training set:  1 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: \\data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "print(\"Training Set of MNIST: \", \"\\n\")\n",
        "print(\"Type of object that holds training data:\")\n",
        "print(type(mnist_train),\"\\n\")\n",
        "print(type(mnist_train[0]))\n",
        "print(\"Length of Tuple: in Torchvision Object: \" ,\"\\n\" , len(mnist_train[0]),\"\\n\")\n",
        "print(\"Image: \")\n",
        "print(type(mnist_train[0][0]),\"\\n\")\n",
        "print(\"Shape of Image:\", \"\\n\", mnist_train[0][0].shape,\"\\n\")\n",
        "print(\"Class: \")\n",
        "print(type(mnist_train[0][1]))\n",
        "print(\"Length of training set: \",len(mnist_train[0][0]),\"\\n\")\n",
        "mnist_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tVUGycwceZ7"
      },
      "source": [
        "##### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG3U1Vkdcefa",
        "outputId": "63c8f2c0-1932-4643-8fa9-a3e8535c2791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Set of MNIST:  \n",
            "\n",
            "Type of object that holds testing data:\n",
            "<class 'torchvision.datasets.mnist.MNIST'> \n",
            "\n",
            "<class 'torchvision.datasets.mnist.MNIST'> \n",
            "\n",
            "<class 'tuple'>\n",
            "Length of Tuple: in Torchvision Object:  \n",
            " 2 \n",
            "\n",
            "Image: \n",
            "<class 'torch.Tensor'> \n",
            "\n",
            "Shape of Image: \n",
            " torch.Size([1, 28, 28]) \n",
            "\n",
            "Class: \n",
            "<class 'int'>\n",
            "Length of training set:  1 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: \\data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "print(\"Testing Set of MNIST: \", \"\\n\")\n",
        "print(\"Type of object that holds testing data:\")\n",
        "print(type(mnist_train),\"\\n\")\n",
        "print(type(mnist_test),\"\\n\")\n",
        "print(type(mnist_test[0]))\n",
        "print(\"Length of Tuple: in Torchvision Object: \" ,\"\\n\" , len(mnist_test[0]),\"\\n\")\n",
        "print(\"Image: \")\n",
        "print(type(mnist_test[0][0]),\"\\n\")\n",
        "print(\"Shape of Image:\", \"\\n\", mnist_test[0][0].shape,\"\\n\")\n",
        "print(\"Class: \")\n",
        "print(type(mnist_test[0][1]))\n",
        "print(\"Length of training set: \",len(mnist_test[0][0]),\"\\n\")\n",
        "mnist_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBBoRk8Acel8"
      },
      "source": [
        "##### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foTTNd3XcesK",
        "outputId": "80a288c6-d5c7-43eb-eacc-3be4cadef0da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set of CIFAR:  \n",
            "\n",
            "Type of object that holds training data:\n",
            "<class 'torchvision.datasets.mnist.MNIST'> \n",
            "\n",
            "<class 'tuple'>\n",
            "Length of Tuple: in Torchvision Object:  \n",
            " 2 \n",
            "\n",
            "Image: \n",
            "<class 'torch.Tensor'> \n",
            "\n",
            "Shape of Image: \n",
            " torch.Size([1, 28, 28]) \n",
            "\n",
            "Class: \n",
            "<class 'int'>\n",
            "Length of training set:  1 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: \\data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "print(\"Training Set of CIFAR: \", \"\\n\")\n",
        "print(\"Type of object that holds training data:\")\n",
        "print(type(cifar_train),\"\\n\")\n",
        "print(type(cifar_train[0]))\n",
        "print(\"Length of Tuple: in Torchvision Object: \" ,\"\\n\" , len(cifar_train[0]),\"\\n\")\n",
        "print(\"Image: \")\n",
        "print(type(cifar_train[0][0]),\"\\n\")\n",
        "print(\"Shape of Image:\", \"\\n\", cifar_train[0][0].shape,\"\\n\")\n",
        "print(\"Class: \")\n",
        "print(type(cifar_train[0][1]))\n",
        "print(\"Length of training set: \",len(cifar_train[0][0]),\"\\n\")\n",
        "cifar_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqZmps_lc2jR"
      },
      "source": [
        "##### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kzQ3de5c22z",
        "outputId": "9411f98f-428f-4681-c352-c244cc0c0869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Set of CIFAR:  \n",
            "\n",
            "Type of object that holds testing data:\n",
            "<class 'torchvision.datasets.mnist.MNIST'> \n",
            "\n",
            "<class 'torchvision.datasets.mnist.MNIST'> \n",
            "\n",
            "<class 'tuple'>\n",
            "Length of Tuple: in Torchvision Object:  \n",
            " 2 \n",
            "\n",
            "Image: \n",
            "<class 'torch.Tensor'> \n",
            "\n",
            "Shape of Image: \n",
            " torch.Size([1, 28, 28]) \n",
            "\n",
            "Class: \n",
            "<class 'int'>\n",
            "Length of training set:  1 \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: \\data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "print(\"Testing Set of CIFAR: \", \"\\n\")\n",
        "print(\"Type of object that holds testing data:\")\n",
        "print(type(cifar_test),\"\\n\")\n",
        "print(type(cifar_test),\"\\n\")\n",
        "print(type(cifar_test[0]))\n",
        "print(\"Length of Tuple: in Torchvision Object: \" ,\"\\n\" , len(cifar_test[0]),\"\\n\")\n",
        "print(\"Image: \")\n",
        "print(type(cifar_test[0][0]),\"\\n\")\n",
        "print(\"Shape of Image:\", \"\\n\", cifar_test[0][0].shape,\"\\n\")\n",
        "print(\"Class: \")\n",
        "print(type(cifar_test[0][1]))\n",
        "print(\"Length of training set: \",len(cifar_test[0][0]),\"\\n\")\n",
        "cifar_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzaCwP9AMiFY"
      },
      "source": [
        "##### PyTorch Image Classification Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0eJffmLMKBB"
      },
      "source": [
        "##### Model 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo4z1VjxMKLr",
        "outputId": "3e6451fd-69de-4b3a-938c-ace2b74562b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 26, 26]             320\n",
            "            Conv2d-2           [-1, 64, 24, 24]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 12, 12]               0\n",
            "           Dropout-4           [-1, 64, 12, 12]               0\n",
            "           Flatten-5                 [-1, 9216]               0\n",
            "            Linear-6                  [-1, 128]       1,179,776\n",
            "            Linear-7                   [-1, 10]           1,290\n",
            "           Softmax-8                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.66\n",
            "Params size (MB): 4.58\n",
            "Estimated Total Size (MB): 5.24\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        }
      ],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1,32,kernel_size=3),\n",
        "    nn.Conv2d(32,64,kernel_size = 3),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Dropout(.25),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(9216,128),\n",
        "    nn.Linear(128,10),\n",
        "    nn.Softmax()\n",
        "    )\n",
        "summary(model, (1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM3wYG0JMKRq"
      },
      "source": [
        "##### Model 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeGTiDluMKXp",
        "outputId": "deab6429-0a00-4cb7-8a84-706c262a32d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 27, 27]             160\n",
            "         MaxPool2d-2           [-1, 32, 13, 13]               0\n",
            "            Conv2d-3           [-1, 64, 12, 12]           8,256\n",
            "         MaxPool2d-4             [-1, 64, 6, 6]               0\n",
            "            Conv2d-5            [-1, 128, 5, 5]          32,896\n",
            "           Dropout-6            [-1, 128, 5, 5]               0\n",
            "         MaxPool2d-7            [-1, 128, 5, 5]               0\n",
            "            Conv2d-8             [-1, 10, 3, 3]          11,530\n",
            "           Flatten-9                   [-1, 90]               0\n",
            "           Linear-10                   [-1, 10]             910\n",
            "================================================================\n",
            "Total params: 53,752\n",
            "Trainable params: 53,752\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.38\n",
            "Params size (MB): 0.21\n",
            "Estimated Total Size (MB): 0.59\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model1 = nn.Sequential(\n",
        "    \n",
        "         nn.Conv2d(1,32,kernel_size = 2),\n",
        "         nn.MaxPool2d(2,2),\n",
        "         nn.Conv2d(32,64,kernel_size = 2),\n",
        "         nn.MaxPool2d(2,2),\n",
        "         nn.Conv2d(64,128,kernel_size = 2),\n",
        "         nn.Dropout(0.25),\n",
        "         nn.MaxPool2d(1,1),\n",
        "         nn.Conv2d(128,10,kernel_size = 3),\n",
        "         nn.Flatten(),\n",
        "         nn.Linear(90,10)\n",
        "         \n",
        "        )\n",
        "summary(model1,(1,28,28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU4hhQsLMKdY"
      },
      "source": [
        "##### Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlCDLdp9MKj4",
        "outputId": "f68d8435-af4e-4ebe-89ef-866bd4df8a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 26, 26]             320\n",
            "            Conv2d-2           [-1, 64, 24, 24]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 12, 12]               0\n",
            "           Dropout-4           [-1, 64, 12, 12]               0\n",
            "           Flatten-5                 [-1, 9216]               0\n",
            "            Linear-6                  [-1, 128]       1,179,776\n",
            "            Linear-7                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.66\n",
            "Params size (MB): 4.58\n",
            "Estimated Total Size (MB): 5.24\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model2 = nn.Sequential(\n",
        "    nn.Conv2d(1,32,kernel_size =3),\n",
        "    nn.Conv2d(32,64,kernel_size = 3),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Dropout(0.25),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(9216,128),\n",
        "    nn.Linear(128,10))\n",
        "summary(model2,(1,28,28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCiRLXjFP2B6"
      },
      "source": [
        "#### Pretrained Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6_Hm2V3AaWV"
      },
      "source": [
        "##### Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "d6973985c2814ed19fab8666f9185d19",
            "86759c86dcfa41e68d4b336d52bf4946",
            "d0dc802298834f6dbd7f71657d82d12b",
            "ef08f267f87b433fb4e1a0300b4c3d44",
            "cc3d004516684651b55333e01ba89e4c",
            "680a96beb2b0445c94d343dbc6b7f648",
            "862e1e72d698452a9c74b4b469d6e49d",
            "9f850c13215c4593bc54024c4e06e123",
            "7f438747f1b642f1856758370c5283b2",
            "51831d166254487abe127e8d114c1d3f",
            "0f6aff92d1e148a198b34c50f995bf00"
          ]
        },
        "id": "iSfHZR7mAadC",
        "outputId": "860cc3a5-68d2-46aa-dca6-9e2386fee10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6973985c2814ed19fab8666f9185d19"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pre_trained_one = resnet50(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkVimzy-1ebb"
      },
      "source": [
        "##### Inception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "7a4af8d435d5463aa8eea4db92e3319b",
            "fedb9ab52ee9469f96dbbef72f6eacb0",
            "aacd2e20b7c447909fd6e2b979e1cd28",
            "53ff99656da7430a9e623f37b57ad1bf",
            "f3cf93b1cf4e4e74ab4aa84b82aea5e0",
            "8ba631bda1c346cfa6963b7ccc46d44a",
            "de13b92e912f4ad990be99eacd1c1ad3",
            "a492caffa36e47df9d26260ab97e48c6",
            "f4ad0317710f439da2c884e9e7036704",
            "6710675d71f64d14a7cfaaf8ca10bd2b",
            "ebaa38383c034ffe8496cfae083fbcc5"
          ]
        },
        "id": "kRrJ4cGkP2P7",
        "outputId": "33bc661f-601c-4107-a601-a1b4204c1da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/104M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a4af8d435d5463aa8eea4db92e3319b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pretrained_two = inception_v3(pretrained = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK_hYDdJ11ah"
      },
      "source": [
        "##### Alexnet "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "6cc8310769ef49c78b33f7da06f78dfe",
            "526ef982275f47bf9e027e4b6aeea141",
            "653731e065c34c4a82b3aa8536b6f9fb",
            "28ee13cb005548fa95191d0bad78091e",
            "4743792f3d184261aaf1ed895a23f2ec",
            "8ee0f0ea73ed4797807e7b92fd33c6db",
            "de9796c101944567b3ddff846b92c1dc",
            "36ab5bd67036402f86aab5ad976d6053",
            "c0b4ae60ab104c4390b3f848f9157d45",
            "86d8d831c11b4b259676f5fb5544a7ef",
            "a7c0569cbf6d4412afcf5b3b9d419d40"
          ]
        },
        "id": "LN1JNGfl11ka",
        "outputId": "7cc8a8ba-4b2c-4974-8664-58232cf3e77e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6cc8310769ef49c78b33f7da06f78dfe"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pretrained_three = alexnet(pretrained = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo02W_IoroRr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Training loop \n",
        "'''\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)   \n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt8NZtwTD6p7"
      },
      "source": [
        "##### Fit Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMBOLkxZECgg"
      },
      "source": [
        "##### Model 1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeRJJmP4gKIS",
        "outputId": "5c0cd2ab-46b9-4051-f4d8-2aa171498a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.302886  [    0/60000]\n",
            "loss: 1.825113  [ 6400/60000]\n",
            "loss: 1.632869  [12800/60000]\n",
            "loss: 1.674819  [19200/60000]\n",
            "loss: 1.566327  [25600/60000]\n",
            "loss: 1.510678  [32000/60000]\n",
            "loss: 1.572330  [38400/60000]\n",
            "loss: 1.532563  [44800/60000]\n",
            "loss: 1.517643  [51200/60000]\n",
            "loss: 1.596279  [57600/60000]\n"
          ]
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "train_loop(mnist_train_dataloader,model,loss,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Yez8tOreMC"
      },
      "source": [
        "##### Model 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvktnVaIreTS",
        "outputId": "6672cdb0-4279-4e92-82e6-5ff2cdb16171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.308281  [    0/60000]\n",
            "loss: 0.594093  [ 6400/60000]\n",
            "loss: 0.344537  [12800/60000]\n",
            "loss: 0.210261  [19200/60000]\n",
            "loss: 0.253933  [25600/60000]\n",
            "loss: 0.282509  [32000/60000]\n",
            "loss: 0.087234  [38400/60000]\n",
            "loss: 0.103404  [44800/60000]\n",
            "loss: 0.390012  [51200/60000]\n",
            "loss: 0.101649  [57600/60000]\n"
          ]
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model1.parameters(), lr=0.01, momentum=0.9)\n",
        "train_loop(mnist_train_dataloader,model1,loss,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76vWffK1rea2"
      },
      "source": [
        "##### Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1ecn0nDreh3",
        "outputId": "e7210dff-74ad-45aa-976a-85fbc7d4f797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.303354  [    0/60000]\n",
            "loss: 0.140123  [ 6400/60000]\n",
            "loss: 0.330829  [12800/60000]\n",
            "loss: 0.152062  [19200/60000]\n",
            "loss: 0.255621  [25600/60000]\n",
            "loss: 0.067460  [32000/60000]\n",
            "loss: 0.253748  [38400/60000]\n",
            "loss: 0.111656  [44800/60000]\n",
            "loss: 0.051700  [51200/60000]\n",
            "loss: 0.213760  [57600/60000]\n"
          ]
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model2.parameters(), lr=0.01, momentum=0.9)\n",
        "train_loop(mnist_train_dataloader,model2,loss,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go6lGy-tEJ0u"
      },
      "source": [
        "##### Pretrained Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQz2jQwpRHZM"
      },
      "source": [
        "##### Resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBAcDSy6RHfr",
        "outputId": "e0f9ecac-8f9a-4258-ad8c-1ef2e2e0bc3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 12.358508  [    0/60000]\n",
            "loss: 0.253075  [ 6400/60000]\n",
            "loss: 0.259036  [12800/60000]\n",
            "loss: 0.165950  [19200/60000]\n",
            "loss: 0.132990  [25600/60000]\n",
            "loss: 0.158551  [32000/60000]\n",
            "loss: 0.026966  [38400/60000]\n",
            "loss: 0.103776  [44800/60000]\n",
            "loss: 0.025783  [51200/60000]\n",
            "loss: 0.060134  [57600/60000]\n"
          ]
        }
      ],
      "source": [
        "optimizer = optim.SGD(pre_trained_one.parameters(), lr=0.01, momentum=0.9)\n",
        "train_loop(mnist_train_dataloader,pre_trained_one,loss,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbpFmPJNRHoh"
      },
      "source": [
        "##### Inception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TREeOh_FRHuP",
        "outputId": "c949a108-34d9-46d4-f472-a4a11cbd030d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b027ead014ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_two\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_train_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpretrained_two\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-c92ab892aba1>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mInceptionOutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0maux_defined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_5d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# N x 288 x 35 x 35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_6a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;31m# N x 768 x 17 x 17\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_6b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mbranch3x3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch3x3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mbranch3x3dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch3x3dbl_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (1 x 1). Kernel size: (3 x 3). Kernel size can't be greater than actual input size"
          ]
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(pretrained_two.parameters(), lr=0.01, momentum=0.9)\n",
        "train_loop(mnist_train_dataloader,pretrained_two,loss,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH-C2ZTcRH1j"
      },
      "source": [
        "Alexnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW-5W6fCRH7h",
        "outputId": "feadbb2f-5306-46b3-fbf8-e65a29e236d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-0c5a8eb56097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_three\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_train_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpretrained_three\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-c92ab892aba1>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/alexnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    167\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                             return_indices=self.return_indices)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (192x2x2). Calculated output size: (192x0x0). Output size is too small"
          ]
        }
      ],
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(pretrained_three.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "train_loop(mnist_train_dataloader,pretrained_three,loss,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yqur-ZUOEGL"
      },
      "source": [
        "#### Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RuPXdENsdeH"
      },
      "source": [
        "##### Small model that updates parameters with PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d28cn7e8OEGL"
      },
      "outputs": [],
      "source": [
        "def apply_step(params,prn=True):\n",
        "    speed = time*3 + (time-9.5)**2 + 1\n",
        "    a,b,c = params \n",
        "    pred = a*(time**2) + b*time + c\n",
        "    loss = ((pred - speed)**2).mean()\n",
        "    loss.backward()\n",
        "    lr = 1e-5\n",
        "    params.grad\n",
        "    params.data -= lr * params.grad.data\n",
        "    params.grad=None\n",
        "    if prn: print(loss.item())\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ofk5t1Eja8U"
      },
      "source": [
        "##### User inputs size and a matrix of weights is created with gradient being applied to functionality of matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiCki_tJjbFb"
      },
      "outputs": [],
      "source": [
        "def init_weights(size):\n",
        "    weights = (torch.randn(size)).requires_grad_()\n",
        "    return weights\n",
        "\n",
        "weights = init_weights(5)\n",
        "weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF1mTE5cj-zW"
      },
      "source": [
        "##### Bias is applied to Artificial Neural Networks before each hidden layer and output layer to scale the input to relevant numerical value. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfhOF0Aij-64"
      },
      "outputs": [],
      "source": [
        "def bias():\n",
        "    bias = torch.randn(1)\n",
        "    return bias\n",
        "\n",
        "bias = bias()\n",
        "bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqpeNj-pjz6P"
      },
      "source": [
        "##### Input weights and learning rate. Set new variable of weights by taking gradient and multiplying by learning rate (to scale). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSzRNxf3j0A-"
      },
      "outputs": [],
      "source": [
        "def update(lr,weights):\n",
        "    new_weights -= weights.grad * lr\n",
        "    return new_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwrcLj7CRA_l"
      },
      "source": [
        "##### L1 Loss "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LktCPECQEycA"
      },
      "outputs": [],
      "source": [
        "def L1_loss(average,real):\n",
        "    result = (average - real).abs().mean()\n",
        "    return result\n",
        "\n",
        "L1_loss(average_3s,stacked_3s[0])\n",
        "L1_loss(average_7s,stacked_7s[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pO3mitPimQC"
      },
      "source": [
        "\n",
        "##### Tabular Data\n",
        "$\\sum\\limits_{i=1}^{n} |y_t - y_i|$\n",
        "\n",
        "##### Image Data\n",
        "$\\sum\\limits_{i=1}^{n} |\\overrightarrow{y_t} - \\overrightarrow{y_i}|$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1rXQ1nlj0gW"
      },
      "source": [
        "##### Mean Squared Error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78BvVsd3E5OY"
      },
      "outputs": [],
      "source": [
        "def mean_sq_error_loss(average,real):    \n",
        "    result = ((average-real)**2).sqrt().mean()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjSKfJuujXHZ"
      },
      "source": [
        "\n",
        "Tabular Data \n",
        "\n",
        "$\\sum\\limits_{i=1}^{n} \\sqrt{(y_t - y_i)^2}$\n",
        "\n",
        "Image Data\n",
        "\n",
        "$\\sum\\limits_{i=1}^{n} \\sqrt{(\\overrightarrow{y_t} - \\overrightarrow{y_i})^2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VmulNIMM-yd"
      },
      "source": [
        "##### Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1OYn8DvEreI"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+torch.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLoGiNWvnih-"
      },
      "source": [
        "\n",
        "##### Images\n",
        "$ 1.   f(x;\\theta) = \\frac{1}{1 + \\exp\\left(-\\theta^{T} x\\right)} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_J48LTGMVEx"
      },
      "source": [
        "##### Tabular \n",
        "$2.  f(x) = \\frac{1}{1 + \\exp\\left(- x\\right)} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "2DiG2Aq3K267",
        "outputId": "1b767f98-e093-4eae-a802-39d7cbcaab68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Sigmoid')"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "x = torch.arange(-5,5,0.1)\n",
        "y = sigmoid(x)\n",
        "plt.plot(x,y)\n",
        "plt.title(\"Sigmoid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWec6g6-7Gsg"
      },
      "source": [
        "##### Loading many images from a folder, opening them using PIL, transforming into tensors. Stacking the tensors created from the folder, and converting values in each tensor to float. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfxJuY3f7GyD"
      },
      "outputs": [],
      "source": [
        "def load_data(folder_name):\n",
        "    training_tensor = [tensor(Image.open(i)) for i in folder_name]\n",
        "    training_stack = ((torch.stack(training_tensor)).float())\n",
        "    return training_stack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L26fm1G97G3k"
      },
      "source": [
        "##### Concatinating different training stacks. Takes in multiple stacked tensors and creates another stacked tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP8rcKxo7G8p"
      },
      "outputs": [],
      "source": [
        "def training_data(*args):\n",
        "    training = (torch.cat(args))\n",
        "    return training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o3HJ3HT7HCr"
      },
      "source": [
        "##### Takes in object from load_data method. Returns row and column shape of the Tensor. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKQqcai17HIX"
      },
      "outputs": [],
      "source": [
        "def size(training_stack):\n",
        "    size = ((training_stack.shape)[1]) * (training_stack.shape[2])\n",
        "    return size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAFL3a4p7Hir"
      },
      "source": [
        "##### Return shape of data that is a stock of tensors. The size of the images. TBD if this is correct **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuOjgw0n7Hn7"
      },
      "outputs": [],
      "source": [
        "def transform_data_for_model(training_stack):\n",
        "    result = training_stack[1] * training_stack[2]\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5sM_yfT7Hsk"
      },
      "source": [
        "##### Multiplying a group of weights by a training tensor while adding bias. Tensor must be of shape 784. TBD**   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7l6oEjGU7HxQ"
      },
      "outputs": [],
      "source": [
        "def matrix_multiply(training_stack):\n",
        "    new_training_stack = (training_stack).view(-1,784)\n",
        "    pred = ((new_training_stack) @ weights) + bias\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrRZTw1r7IAZ"
      },
      "source": [
        "##### Mean Absolute Error Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ch7tMLx7IGF"
      },
      "outputs": [],
      "source": [
        "def loss(pred,target):\n",
        "    loss = (pred-target).abs().mean()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J41eqwnT7IU8"
      },
      "source": [
        "##### Returning shape of images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZyl459M7IZ4"
      },
      "outputs": [],
      "source": [
        "def size_of_image(image):\n",
        "    image_size = image.shape\n",
        "    return image_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfjNmGFJ7Ie1"
      },
      "source": [
        "##### Convolving over an image. Given image shape and kernel as a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HqIiRtm7Ikj"
      },
      "outputs": [],
      "source": [
        "def apply_kernel(row,col,kernel):\n",
        "    convolution = (img[row-1:row+2,col-1:col+2] * kernel).sum()\n",
        "    return convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNa0KBfO7Ipg"
      },
      "source": [
        "##### Using Apply Kernel Function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqgJdBdQ7Iv8"
      },
      "outputs": [],
      "source": [
        "def convolution_top(x,y):\n",
        "    rng = (x,y)\n",
        "    top_edge = tensor([[apply_kernel(i,j,top_edge) for j in rng] for i in rng])\n",
        "    return top_edge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVWTAnrI7I0t"
      },
      "source": [
        "##### Calculating row shape after applying symmetrical kernel to image. TBD** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui4cjIh87I7R"
      },
      "outputs": [],
      "source": [
        "def row(padding, stride, height):\n",
        "    new_row = (height + padding) // stride\n",
        "    return new_row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cylQX_A7JA7"
      },
      "source": [
        "##### Calculating column shape after applying symmetrical kernel to image.  TBD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN4OHEc87JFY"
      },
      "outputs": [],
      "source": [
        "def column(padding,stride,height):\n",
        "    new_column = (height + padding) // stride\n",
        "    return new_column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0cVVK9t7JWT"
      },
      "source": [
        "##### Calculating output shape. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CnECBP57Ja_"
      },
      "outputs": [],
      "source": [
        "def output_shape(w,n,p,f):\n",
        "    output = int((W - K + (2*P))/(S + 1))\n",
        "    new_output = (w - n + (2*p) - f) + 1\n",
        "    return new_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKrlFyGh7JhP"
      },
      "source": [
        "##### Top line kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9KT6xTM7JmL"
      },
      "outputs": [],
      "source": [
        "def top_edge():\n",
        "    top_edge = (tensor([1,1,1],[0,0,0],[-1,-1,-1])).float()\n",
        "    return top_edge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ymh-nQe97JrZ"
      },
      "source": [
        "##### Bottom Line Kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlKc3Pai7Jx5"
      },
      "outputs": [],
      "source": [
        "def bottom_edge():\n",
        "    bottom_edge = (tensor([-1,-1,-1],[0,0,0],[1,1,1])).float()\n",
        "    return bottom_edge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QIx9NoC7J24"
      },
      "source": [
        "##### Right Line Kernel. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhmFeE237J8W"
      },
      "outputs": [],
      "source": [
        "def right_edge():\n",
        "    right_edge = (tensor([-1,0,1],[-1,0,1],[-1,0,1])).float()\n",
        "    return right_edge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-YPcZM87KBi"
      },
      "source": [
        "##### Left Line Kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWWdyz4m7KHQ"
      },
      "outputs": [],
      "source": [
        "def left_edge():\n",
        "    left_edge = (tensor([1,0,-1],[1,0,-1],[1,0,-1])).float()\n",
        "    return left_edge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V02cctFf7KL9"
      },
      "source": [
        "##### Left to Right Diagonal Line. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sCywo2e7KSt"
      },
      "outputs": [],
      "source": [
        "def diag1_edge():\n",
        "    diag1_edge = (tensor([1,0,-1],[0,1,0],[-1,0,1])).float()\n",
        "    return diag1_edge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY_3-oXz_i7L"
      },
      "source": [
        "##### Parameters to be updated for model, given size as an input. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xTvZ6AFOEGM"
      },
      "outputs": [],
      "source": [
        "def init_params(size,std=1.0): \n",
        "    params = (torch.randn(size)*std).requires_grad_() \n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tn7ZFk14fM0"
      },
      "source": [
        "##### Matrix Multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X64Pm-t4fV4"
      },
      "outputs": [],
      "source": [
        "def linear1(xb,weights,bias):\n",
        "    result = xb@weights + bias\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHB4-6914jQr"
      },
      "source": [
        "##### Iterates through variables that stores images and converts to image object, then tensor and saves in list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtQUT8eg4jY-"
      },
      "outputs": [],
      "source": [
        "stacked_valid3s = [tensor(Image.open(o)) for o in valid_3s]\n",
        "stacked_valid7s = [tensor(Image.open(o)) for o in valid_7s]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIqKKjAv4jhE"
      },
      "source": [
        "##### Converts tensors saved in list and saves them into a tensor (stacked) and converts all values to floats, then scales. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD6SjpgW4joX"
      },
      "outputs": [],
      "source": [
        "stacked_3s = torch.stack(stacked_3s).float()/255\n",
        "stacked_7s = torch.stack(stacked_7s).float()/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcoGriwU4jwK"
      },
      "source": [
        "##### Gets the average of all Tensors in the stacked tensor object. (Training data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJv5yK2v4j3f"
      },
      "outputs": [],
      "source": [
        "average_3s = (stacked_3s).mean(0)\n",
        "average_7s = (stacked_7s).mean(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEQg7eCF4kAU"
      },
      "source": [
        "##### Gets the average of all Tensors in the stacked tensor object. (Validation Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ngHFKbT4kI6"
      },
      "outputs": [],
      "source": [
        "average_valid3s = (stacked_valid3s).mean(0)\n",
        "average_valid7s = (stacked_valid7s).mean(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPlfrkRW4kQu"
      },
      "source": [
        "##### Calculates loss of first image in training data and first image in 'averaged training data' using L1 technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaCVl-fk4kYj",
        "outputId": "c41230dc-d03f-4b6d-b858-368cabd69b1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(38.8817)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
>>>>>>> 66b3c37f171fc821718eb029fe3992db7ecf19ad
      ],
      "source": [
        "L1_loss(average_3s,stacked_3s[0])\n",
        "L1_loss(average_7s,stacked_7s[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih-jzDW34kg3"
      },
      "source": [
        "##### Calculates loss of first image in validation data and first image in 'averaged validation data' using L1 technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY495cZZ4kpe"
      },
      "outputs": [],
      "source": [
        "L1_loss(average_valid_3s,stacked_valid_3s[0])\n",
        "L1_loss(average_valid_7s,stacked_valid_7s[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJBtLIME4w8e"
      },
      "source": [
        "\n",
        "##### Calculates loss of first image in training data and first image in 'averaged training data' using Mean Squared Error technique.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaubwtJu4xDN",
        "outputId": "e055a714-9018-4c4b-abef-3201f565aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(38.8817)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean_sq_error_loss(average_3s,stacked_3s[0])\n",
        "mean_sq_error_loss(average_7s,stacked_7s[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFrPuFTK4xJu"
      },
      "source": [
        "##### Calculates loss of first image in validation data and first image in 'averaged validation data' using Mean Squared Error technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQwGsoRl4xQf"
      },
      "outputs": [],
      "source": [
        "mean_sq_error_loss(average_valid_3s,stacked_valid_3s[0])\n",
        "mean_sq_error_loss(average_valid_7s,stacked_valid_7s[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lJib8pi4xXA"
      },
      "source": [
        "### Pytorch Out of The Box Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b8VWuBIrv45"
      },
      "source": [
        "##### L1 Loss function applied to training data and averaged training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10numEJq4xdj",
        "outputId": "3bce5e00-63e0-4137-ec74-462c770a7d5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(38.8817)"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.l1_loss(average_3s,stacked_3s[0])\n",
        "F.l1_loss(average_7s,stacked_7s[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je6fKGTk4xlW"
      },
      "source": [
        "##### L1 Loss function applied to validation data and averaged validation data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4Fr1O6I4xsF"
      },
      "outputs": [],
      "source": [
        "F.l1_loss(average_valid_3s,stacked_valid_3s[0])\n",
        "F.l1_loss(average_valid_7s,stacked_valid_7s[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxVjrrtj4xzY"
      },
      "source": [
        "##### Mean Squared Error Loss function applied to training data and averaged training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsIQ4uGB4x6r",
        "outputId": "244bdec2-81f3-4314-8cf6-1d8e49f87aaf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(8844.8926)"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.mse_loss(average_3s,stacked_3s[0])\n",
        "F.mse_loss(average_7s,stacked_7s[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DD_xxFf5NXc"
      },
      "source": [
        "##### Mean Squared Error Loss function applied to validation data and averaged validation data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyt5aYBW5NeN"
      },
      "outputs": [],
      "source": [
        "F.mse_loss(average_valid_3s,stacked_valid_3s[0])\n",
        "F.mse_loss(average_valid_7s,stacked_valid_7s[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRjbEL5E5Nke"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw7yK5Ye5Nqu"
      },
      "outputs": [],
      "source": [
        "weights = init_params(28*28,1)\n",
        "bias = init_params(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5SO1VvL5Nxf"
      },
      "source": [
        "##### Calculating Predictions based on MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stpKXm-65N3v"
      },
      "outputs": [],
      "source": [
        "def mnist_loss(predictions,targets):\n",
        "    predictions=predictions.sigmoid()\n",
        "    return torch.where(targets==1,1-predictions,predictions).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n40N5xAp7KeL"
      },
      "source": [
        "##### Given a model, calculate loss and take derivative of loss function w.r.t weights for updating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e4fjRKf5OqQ"
      },
      "outputs": [],
      "source": [
        "def calc_grad(xb,yb,model):\n",
        "    preds=model(xb)\n",
        "    loss = mnist_loss(preds,yb)\n",
        "    loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRtpB1hZ5OwP"
      },
      "source": [
        "##### Given a dataloader object, learning rate, and parameters; iterate through (examples and targets) dataloader, take derivative of loss w.r.t. weights, and iterate through parameters and update each parameters according to learning rate and gradient calculated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u75JwjF35O8O"
      },
      "source": [
        "##### Training a model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbzG_Hmi5PB8"
      },
      "outputs": [],
      "source": [
        "def train_model(model,epochs):\n",
        "    for i in range(epochs):\n",
        "        train_epoch(model)\n",
        "        print(validate_epoch(model),end=' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFNk_as552qd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stks4ZVi521I"
      },
      "outputs": [],
      "source": [
        "def batch_accuracy(xb,yb):\n",
        "    preds = xb.sigmoid()\n",
        "    correct = (preds>.5)==yb\n",
        "    return correct.float().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSY-wssv53Fz"
      },
      "source": [
        "##### Getting batch accuracy of model, based on dataloader data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZbncgVF53Lf"
      },
      "outputs": [],
      "source": [
        "def validate_epoch(model):\n",
        "    accs = [batch_accuracy(model(xb),yb) for xb,yb in valid_dl]\n",
        "    return round(torch.stack(accs).mean().item(),4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58YWz8-H53Q9"
      },
      "source": [
        "###### Matrix multiplication in PyTorch. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mI267bn53Wb"
      },
      "outputs": [],
      "source": [
        "def simple_net(xb):\n",
        "    res = xb@w1 + b1\n",
        "    res = res.max(tensor(0.0))\n",
        "    res = res@w2 + b2\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jr_zAjtCzCA"
      },
      "source": [
        "###### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TazmVhdCzKF"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)   \n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-LYVfRjCtJL"
      },
      "source": [
        "##### Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h9TJGDDCtQu"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkAeircAhV9b"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDdQArMUhWPY"
      },
      "outputs": [],
      "source": [
        "class Training:\n",
        "\n",
        "    def __init__(self,dataloader,batch):\n",
        "        self.dataloader = dataloader\n",
        "\n",
        "    def predict(model,loss):\n",
        "        for batch, (x,y) in enumerate(dataloader):\n",
        "            pred = model(x)\n",
        "            result = loss(pred,y)\n",
        "    \n",
        "    def measure(optimizer):\n",
        "        # zero out optimizer?\n",
        "        optimizer.zero_grad()\n",
        "        result.backward()\n",
        "        optimizer.step()\n",
        "        loss.item()\n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wki0xltw5yoR"
      },
      "source": [
        "##### Class for updating weights. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPLGm3SU53iq"
      },
      "outputs": [],
      "source": [
        "class BasicOptim:\n",
        "    \n",
        "    def __init__(self,params,lr): \n",
        "        self.params,self.lr = list(params),lr\n",
        "        \n",
        "    def step(self,*args,**kwargs):\n",
        "        for p in self.params:\n",
        "            p.data-=p.grad.data *self.lr\n",
        "    \n",
        "    def zero_grad(self,*args,**kwargs):\n",
        "        for p in self.params:\n",
        "            p.grad = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYTIcw4YeTXm"
      },
      "source": [
        "##### Class for Neural Network: 200 feature inputs, 10 outputs, Relu and Softmax for activation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qqTyuyleTfK",
        "outputId": "8d64b8b0-dc26-4435-d87e-aecad06483e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<__main__.Model object at 0x7f47b64d2310>\n"
          ]
        }
      ],
      "source": [
        "class Model():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.linear = torch.nn.Linear(100,200)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(200,10)\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.linear(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "tiny_model = Model()\n",
        "print(tiny_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzelUBk7eTmq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfcbYE0AeTtL"
      },
      "outputs": [],
      "source": [
        "class CNN():\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.conv2d1 = torch.nn.Conv2d(3,32,kernel_size = 3)\n",
        "        self.activation1 = torch.nn.ReLU()\n",
        "        self.conv2d2 = torch.nn.Conv2d(32,64,kernel_size = 3)\n",
        "        self.activation2 = torch.nn.ReLU()\n",
        "        self.maxpool2d = torch.nn.MaxPool2d(32,64,kernel_size = 3)\n",
        "        self.conv2d = torch.nn.Conv2d\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "\n",
        "    def forward(x):\n",
        "\n",
        "        x = self.conv2d1(x)\n",
        "        x = self.activation1(x)\n",
        "        x = self.conv2d2(x)\n",
        "        x = self.activation1(x)\n",
        "        x = self.maxpool2d(x)\n",
        "        x = self.softmax()\n",
        "  \n",
        "cnn = CNN()\n",
        "print(CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmHmC81z0flO"
      },
      "source": [
        "##### Augmentation Technique Libraries and SubModules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD3pDv-t0fyw"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, CenterCrop, ColorJitter, FiveCrop, Grayscale, Pad, RandomAffine, RandomApply, RandomCrop, RandomGrayscale, RandomHorizontalFlip, RandomPerspective, RandomResizedCrop, RandomRotation, RandomVerticalFlip, Resize, TenCrop, GaussianBlur, RandomInvert, RandomPosterize, RandomSolarize, RandomAdjustSharpness, RandomAutocontrast, RandomEqualize\n",
        "from torchvision.transforms import LinearTransformation, Normalize, RandomErasing,ConvertImageDtype\n",
        "from torchvision.transforms import ToPILImage, ToTensor, PILToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZx6kaJ7KuNN"
      },
      "source": [
        "##### Combines Augmentation Technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykMcuV6n0sBK"
      },
      "outputs": [],
      "source": [
        "both_transforms = Compose([CenterCrop(10),ColorJitter(3),FiveCrop(2),Grayscale(),Pad(2),GaussianBlur(3, sigma=(0.1, 2.0),)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSUjnfn6JhHR"
      },
      "source": [
        "##### Augmentation Technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A540_PIX1MCr",
        "outputId": "0b8101c4-cf3b-4ac7-9418-b902b0470ae5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CenterCrop(size=(10, 10))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "center_crop = CenterCrop(10)\n",
        "center_crop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNKOg6xLJiDt"
      },
      "source": [
        "##### Augmentation Technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXxKFK6fHh2L",
        "outputId": "70f1dc79-7c45-4461-ff45-15e1c88c0097"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ColorJitter(brightness=[0.0, 4.0], contrast=None, saturation=None, hue=None)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "color_jitter = ColorJitter(3)\n",
        "color_jitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6TYf9E2Jiqw"
      },
      "source": [
        "##### Augmentation Technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpJ5QlhnHh5y",
        "outputId": "ea3a52cc-fa43-4e6a-a5e2-f899224917aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FiveCrop(size=(2, 2))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "five_crop = FiveCrop(2)\n",
        "five_crop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq_lYygHJjPf"
      },
      "source": [
        "##### Augmentation Technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj8scRPbHnVP",
        "outputId": "1744a4bc-eff7-4702-bbb4-321dfaed5196"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Grayscale(num_output_channels=1)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gray_scale = Grayscale()\n",
        "gray_scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NlOnB_yJjeG"
      },
      "source": [
        "##### Augmentation Technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "958rxbp7HnYV",
        "outputId": "33909587-fc4b-4633-faf5-66625c26f5fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pad(padding=2, fill=0, padding_mode=constant)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pad = Pad(2)\n",
        "pad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtYQhLpiJjw0"
      },
      "source": [
        "##### Augmentation Technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyTI_2WOHxco",
        "outputId": "5ac94245-7bc9-43d0-ad60-aa7dd050feac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gaussian_blur = GaussianBlur(3, sigma = (0.1,2.0))\n",
        "gaussian_blur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyK0awNHK8zz"
      },
      "source": [
        "##### Training Images Automatically Tensors, Using PyTorch Transforms to convert to PIL Image object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_t6RDvHK888",
        "outputId": "d921c286-379a-45a9-90cc-fd757d3c8f09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'torchvision.transforms.transforms.ToPILImage'>\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(type(trainset[0][0]))\n",
        "img = ToPILImage(trainset[0][0])\n",
        "print(type(img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SS7-_RH7z_r"
      },
      "source": [
        "##### Showing One Image from Trainset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "RElr6MWK70Gb",
        "outputId": "2a8369c1-f8d8-48bb-a4f9-fb37b3937283"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torchvision.transforms.transforms.ToPILImage'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f24be1cf990>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "tensor_img = trainset[0][0].permute(1,2,0)\n",
        "plt.imshow(tensor_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7E64VzRK9Fe"
      },
      "source": [
        "##### Creating a subset dataset that holds specified number of images from original dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPc8in8cK9MA"
      },
      "outputs": [],
      "source": [
        "dataset = torch.utils.data.Subset(trainset, indices=torch.arange(10))\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUI9fCW2JkHN"
      },
      "source": [
        "##### Augmentation Technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "7yU6qwJNA9tk",
        "outputId": "df6df606-f7ea-4fa1-e49c-c136e3ddd725"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4d20220e50>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "img = trainset[3][0].permute(1,2,0)\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogp-ijhTPgW0"
      },
      "source": [
        "##### References:\n",
        "- https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
        "- https://github.com/pytorch/tutorials/blob/master/beginner_source/hyperparameter_tuning_tutorial.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiGSu9AJQApo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "FastAI.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d6973985c2814ed19fab8666f9185d19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86759c86dcfa41e68d4b336d52bf4946",
              "IPY_MODEL_d0dc802298834f6dbd7f71657d82d12b",
              "IPY_MODEL_ef08f267f87b433fb4e1a0300b4c3d44"
            ],
            "layout": "IPY_MODEL_cc3d004516684651b55333e01ba89e4c"
          }
        },
        "86759c86dcfa41e68d4b336d52bf4946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_680a96beb2b0445c94d343dbc6b7f648",
            "placeholder": "​",
            "style": "IPY_MODEL_862e1e72d698452a9c74b4b469d6e49d",
            "value": "100%"
          }
        },
        "d0dc802298834f6dbd7f71657d82d12b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f850c13215c4593bc54024c4e06e123",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f438747f1b642f1856758370c5283b2",
            "value": 102530333
          }
        },
        "ef08f267f87b433fb4e1a0300b4c3d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51831d166254487abe127e8d114c1d3f",
            "placeholder": "​",
            "style": "IPY_MODEL_0f6aff92d1e148a198b34c50f995bf00",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 143MB/s]"
          }
        },
        "cc3d004516684651b55333e01ba89e4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "680a96beb2b0445c94d343dbc6b7f648": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "862e1e72d698452a9c74b4b469d6e49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f850c13215c4593bc54024c4e06e123": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f438747f1b642f1856758370c5283b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51831d166254487abe127e8d114c1d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f6aff92d1e148a198b34c50f995bf00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a4af8d435d5463aa8eea4db92e3319b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fedb9ab52ee9469f96dbbef72f6eacb0",
              "IPY_MODEL_aacd2e20b7c447909fd6e2b979e1cd28",
              "IPY_MODEL_53ff99656da7430a9e623f37b57ad1bf"
            ],
            "layout": "IPY_MODEL_f3cf93b1cf4e4e74ab4aa84b82aea5e0"
          }
        },
        "fedb9ab52ee9469f96dbbef72f6eacb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ba631bda1c346cfa6963b7ccc46d44a",
            "placeholder": "​",
            "style": "IPY_MODEL_de13b92e912f4ad990be99eacd1c1ad3",
            "value": "100%"
          }
        },
        "aacd2e20b7c447909fd6e2b979e1cd28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a492caffa36e47df9d26260ab97e48c6",
            "max": 108949747,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4ad0317710f439da2c884e9e7036704",
            "value": 108949747
          }
        },
        "53ff99656da7430a9e623f37b57ad1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6710675d71f64d14a7cfaaf8ca10bd2b",
            "placeholder": "​",
            "style": "IPY_MODEL_ebaa38383c034ffe8496cfae083fbcc5",
            "value": " 104M/104M [00:01&lt;00:00, 91.8MB/s]"
          }
        },
        "f3cf93b1cf4e4e74ab4aa84b82aea5e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ba631bda1c346cfa6963b7ccc46d44a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de13b92e912f4ad990be99eacd1c1ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a492caffa36e47df9d26260ab97e48c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4ad0317710f439da2c884e9e7036704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6710675d71f64d14a7cfaaf8ca10bd2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebaa38383c034ffe8496cfae083fbcc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cc8310769ef49c78b33f7da06f78dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_526ef982275f47bf9e027e4b6aeea141",
              "IPY_MODEL_653731e065c34c4a82b3aa8536b6f9fb",
              "IPY_MODEL_28ee13cb005548fa95191d0bad78091e"
            ],
            "layout": "IPY_MODEL_4743792f3d184261aaf1ed895a23f2ec"
          }
        },
        "526ef982275f47bf9e027e4b6aeea141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee0f0ea73ed4797807e7b92fd33c6db",
            "placeholder": "​",
            "style": "IPY_MODEL_de9796c101944567b3ddff846b92c1dc",
            "value": "100%"
          }
        },
        "653731e065c34c4a82b3aa8536b6f9fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36ab5bd67036402f86aab5ad976d6053",
            "max": 244408911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0b4ae60ab104c4390b3f848f9157d45",
            "value": 244408911
          }
        },
        "28ee13cb005548fa95191d0bad78091e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86d8d831c11b4b259676f5fb5544a7ef",
            "placeholder": "​",
            "style": "IPY_MODEL_a7c0569cbf6d4412afcf5b3b9d419d40",
            "value": " 233M/233M [00:02&lt;00:00, 90.5MB/s]"
          }
        },
        "4743792f3d184261aaf1ed895a23f2ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ee0f0ea73ed4797807e7b92fd33c6db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9796c101944567b3ddff846b92c1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36ab5bd67036402f86aab5ad976d6053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0b4ae60ab104c4390b3f848f9157d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86d8d831c11b4b259676f5fb5544a7ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7c0569cbf6d4412afcf5b3b9d419d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}